model:
  num_layers: 12
  d_model: 768
  nhead: 12
  dim_feedforward: 3072
  dropout: 0.1
  max_seq_length: 2048
training:
  batch_size: 16
  num_epochs: 10
  learning_rate: 5.0e-05
  weight_decay: 0.01
  warmup_steps: 1000
