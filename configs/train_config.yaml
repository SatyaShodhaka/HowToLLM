

# Define model configurations
model:
  num_layers: 12
  d_model: 768
  nhead: 12
  dim_feedforward: 3072
  dropout: 0.1
  context_length: 512

# Training configurations
training:
  batch_size: 16
  epochs: 100
  learning_rate: 1e-4
  weight_decay: 0.01
  warmup_steps: 1000
  model_save_path: data/model/model.pth


dataset:
  tokenizer_dir: data/tokenizer
  vocab_size: 24784
  data_path: data/dataset/telugu.txt