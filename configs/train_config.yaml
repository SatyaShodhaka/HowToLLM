

# Define model configurations
model:
  num_layers: 12
  d_model: 768
  nhead: 12
  dim_feedforward: 3072
  dropout: 0.1
  context_length: 512

# Training configurations
training:
  batch_size: 16
  num_epochs: 10
  learning_rate: 5e-5
  weight_decay: 0.01
  warmup_steps: 1000
  model_save_path: data/model


dataset:
  tokenizer_dir: data/tokenizer
  vocab_size: 32000
  data_path: data/dataset/data.txt